{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-precision Computation in Python\n",
    "\n",
    "Typically, you can use python or any language for basic calculations without worrying about how many bits are used to represent your numbers.  But occasionally, you may need to make very high precision computations in python.  I've written this tutorial to help you prepare for this need.\n",
    "\n",
    "In my case, high-precision calculation has come up in the context of modeling the motion of bodies in the Solar System.  That requires the integration of gravitational equations of motion over many decades.  This requires some understanding of things like machine precision and round-off error, that I otherwise don't need to worry about.\n",
    "\n",
    "By the way, a wonderful resource for precision computing is the Numerial Recipes textbook.  I learned a great deal from that reference, and borrowed heavily from it to make this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Accuracy\n",
    "\n",
    "First of all, it's important to know that python cannot keep track of arbitrarily small values.\n",
    "For example, if you create a variable and assign it the value 1.0, and then add to it 1e-30, the number in memory is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "myval = 1.0\n",
    "print myval\n",
    "myval += 1e-30\n",
    "print myval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, python stores numbers like 1.0 as 64-bit floating point values (so-called `float64`).  You can find out what the machine accuracy of `float64` is using the `np.finfo()` function in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine parameters for float64\n",
      "---------------------------------------------------------------\n",
      "precision =  15   resolution = 1.0000000000000001e-15\n",
      "machep =    -52   eps =        2.2204460492503131e-16\n",
      "negep =     -53   epsneg =     1.1102230246251565e-16\n",
      "minexp =  -1022   tiny =       2.2250738585072014e-308\n",
      "maxexp =   1024   max =        1.7976931348623157e+308\n",
      "nexp =       11   min =        -max\n",
      "---------------------------------------------------------------\n",
      "\n",
      "Number of bits:  64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print np.finfo(1.0)\n",
    "print \"Number of bits: \", np.finfo(np.float64).bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we add 1e-15 or larger, then python will \"notice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "myval = 1.0\n",
    "print myval\n",
    "myval += 2e-15\n",
    "print myval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what's going on here?  Why do I still see 1.0 and not 1.000000000000002?  Well, python doesn't print values at full precision.  You need to ask it to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000000\n",
      "1.0000000000000020\n",
      "1.0000000000000011\n",
      "1.000000000000001\n",
      "X.1...5....0....5..8.0\n",
      "X..........1....1..1.2\n"
     ]
    }
   ],
   "source": [
    "digitlabel = 'X.1...5....0....5..8.0\\nX..........1....1..1.2'\n",
    "\n",
    "print '{0:.16f}'.format(1)\n",
    "print '{0:.16f}'.format(1+2e-15)\n",
    "print '{0:.16f}'.format(1+1e-15)\n",
    "print '{!r}'.format(1+1e-15)       # {!r} means first, call repr() on the value, then print it out\n",
    "print digitlabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, that's much better!  Hmm, maybe the original attempt of adding 1e-30 was actually fine...  Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000000000000000000000000000000\n",
      "1.00000000000000000000000000000000\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print '{0:.32f}'.format(1)\n",
    "print '{0:.32f}'.format(1+1e-30)\n",
    "print '{!r}'.format(1+1e-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope!  Python really can't tell the difference between 1.0 and (1.0+1e-30).  If you'd like to know more details about *why* python can't tell the difference, please see \"Floating-point representation\" below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## longdouble gives higher precision\n",
    "If you need more than 64 bits, then python also has something called `longdouble` that uses 80 bits (well, the actual number of bits used depends on the platform. see: https://docs.scipy.org/doc/numpy-dev/user/basics.types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine parameters for float128\n",
      "---------------------------------------------------------------\n",
      "precision =  18   resolution = 1e-18\n",
      "machep =    -63   eps =        1.08420217249e-19\n",
      "negep =     -64   epsneg =     5.42101086243e-20\n",
      "minexp = -16382   tiny =       3.36210314311e-4932\n",
      "maxexp =  16384   max =        1.18973149536e+4932\n",
      "nexp =       15   min =        -max\n",
      "---------------------------------------------------------------\n",
      "\n",
      "Number of bits:  128\n"
     ]
    }
   ],
   "source": [
    "print np.finfo(np.longdouble)\n",
    "print \"Number of bits: \", np.finfo(np.longdouble).bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, this seems to suggest that `longdouble` has 128 bits...  But that is actually a half-truth.  python does use 128 bits of memory to store the value, BUT, only 80 bits contain values, the rest is just zero-padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can verify that a longdouble will \"notice\" if 1e-18 has been added (but will not notice if 1e-19 is added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000000000000555\n",
      "0.10000000000000000655\n",
      "0.10000000000000000565\n",
      "0.10000000000000001\n",
      "0.10000000000000001\n",
      "X.1...5....0....5..8.0\n",
      "X..........1....1..1.2\n"
     ]
    }
   ],
   "source": [
    "myld = np.longdouble(0.1)\n",
    "myd  = np.float64(0.1)\n",
    "print '{!r}'.format(myld)\n",
    "print '{!r}'.format(myld+1e-18)\n",
    "print '{!r}'.format(myld+1e-19)\n",
    "print '{!r}'.format(myd)          \n",
    "print '{!r}'.format(myd+1e-18)   # a \"regular\" double doesn't notice the 1e-18\n",
    "print digitlabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintaining precision during calculations\n",
    "If we have a `longdouble`, and then do an operation like `sin()` or `sqrt()`, what precision do we end up with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0999999999999996447\n",
      "4.0999999999999996456\n",
      "X.1...5....0....5..8.0\n",
      "X..........1....1..1.2\n",
      "2.0248456731316586057\n"
     ]
    }
   ],
   "source": [
    "myld = np.longdouble(4.1)\n",
    "print '{!r}'.format(myld)\n",
    "print '{!r}'.format(myld+1e-18)\n",
    "print digitlabel\n",
    "print '{!r}'.format(np.sqrt(myld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF: the representation of 4.1 isn't even good to 18 digits... How can that be for an 80-bit longdouble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, what is the right answer?  We can use mpmath to tell us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mpmath settings:\n",
      "  mp.prec = 136               [default: 53]\n",
      "  mp.dps = 40                 [default: 15]\n",
      "  mp.trap_complex = False     [default: False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mpf('2.024845673131658605596679004662654403763166')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mpmath as mp\n",
    "mp.mp.dps = 40   # 40 digits of precision\n",
    "print(mp.mp)\n",
    "mp.sqrt(mp.mpf(4.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soooo, the \"true\" answer (to 40 digits anyway), is:  \n",
    "`2.024845673131658605596679004662654403763166`  \n",
    "using `longdouble`, we find:  \n",
    "`2.0248456731316586057`  \n",
    "`--123456789012345678^`\n",
    "       \n",
    "which disagrees with truth at the 19th decimal place.\n",
    "\n",
    "How about np.sin()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099833416646828157833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mpf('0.09983341664682815783019686785861666773596606')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myld = np.longdouble(0.1)\n",
    "print '{!r}'.format(np.sin(myld))\n",
    "mp.sin(mp.mpf(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the \"true\" answer (to 40 digits) is:  \n",
    "    `0.09983341664682815783019686785861666773596606`  \n",
    "    using `longdouble` we find:  \n",
    "    `0.099833416646828157833`  \n",
    "    `--12345678901234567890^`  \n",
    "Hmm, how can the output of `np.sin()` agree to more digits than the input did?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating-Point Representation\n",
    "From Numerical Recipes:\n",
    "\n",
    "\"In a floating-point representation, a number is represented internally by a sign bit $S$ (interpreted as plus or minus), an exact integer exponent $E$, and an exactly represented binary mantissa $M$. Taken together these represent the number:\n",
    "$$ S \\times M \\times 2^{E-e} $$ \n",
    "where $e$ is the *bias* of the exponent, a fixed integer constant for any given machine and representation.\"\n",
    "\n",
    "For 64-bit `double` values, the exponent takes 11 bits (with $e=1023$), and the mantissa takes 52 bits.  The final bit is the sign bit $S$ (0=positive, 1=negative).  For example, the binary representations of the decimal values 2.0 and 6.5 are:\n",
    "\\begin{align*}\n",
    "0\\,10000000000\\,0000\\,\\mbox{(+ 48 more zeros)} &= +1 \\times 2^{1024-1023} \\times 1.0_2 = 2. \\\\\n",
    "0\\,10000000001\\,1010\\,\\mbox{(+ 48 more zeros)} &= +1 \\times 2^{1025-1023} \\times 1.1010_2 = 6.5\n",
    "\\end{align*}\n",
    "note, the way to read the last item in the row (e.g. $1.1010_2$ for the decimal 6.5 case) is that each digit after the decimal represents 2 to some negative power.  The first 1 is $2^{-1}$.  Then there are zero $2^{-2}$ but one $2^{-3}$.  So we have $2^{-1} + 2^{-3} = 0.5+0.125 = 0.625$.  So we then have $2^{1025-1023}=2^2 = 4$ which gets multiplied by 1.652 to give: $4*1.625 = 6.5$.  Phew!\n",
    "\n",
    "Notice, however, that not all decimal numbers can be represented exactly in binary (with e.g. 64 bits).  For example, consider the decimal number 0.3.  The closest number to 0.3 that can be represented with a 64-bit double is:\n",
    "\n",
    "FIXME\n",
    "\\begin{align*}\n",
    "0\\,\\,\\,01111111101\\,\\,\\,0011001100110011001100110011001100110011001100110011 &= +1 \\times 2^{1021-1023} \\times 1.0011001100110011...0011_2 \\\\\n",
    "= 2^{-2}\\times 1.2000000000000000\\\\ \n",
    "= 2.99999999999999988897769753748E-1\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "See:  http://babbage.cs.qc.cuny.edu/IEEE-754.old/Decimal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2999999999999999888977697537484\n"
     ]
    }
   ],
   "source": [
    "print '{0:.31f}'.format(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2999999999999999888977697537484"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding floats\n",
    "\"Arithmetic among numbers in floating-point representation is not exact, even if the operands happen to be exactly represented (i.e., have exact values in the form of equation 1.1.1). For example, two floating numbers are added by first right-shifting (dividing by two) the mantissa of the smaller (in magnitude) one and simultaneously increasing its exponent until the two operands have the same exponent. Low-order (least significant) bits of the smaller operand are lost by this shifting. If the two operands differ too greatly in magnitude, then the smaller operand is effectively replaced by zero, since it is right-shifted to oblivion.\"\n",
    "\n",
    "This is why, for a 64-bit double, there is no difference between 1.0 and (1.0+1e-30).  They both give 1.0 because the 1e-30 gets \"bit-shifted to oblivion.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
